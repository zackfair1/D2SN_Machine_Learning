{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble methods\n",
    "\n",
    "In this notebook, you will employ **boosting** and **stacking** to combine multiple models into a (hopefully) better one. Here's what you will do.\n",
    " - [Part 1](#1.-Dataset) - Feature engineering\n",
    " - [Part 2](#2.-Baseline-models) - Baseline models\n",
    " - [Part 3](#3.-Gradient-boosting) - Gradient boosting\n",
    " - [Part 4](#4.-Stacking) - Stacking\n",
    "\n",
    "Let's first import all the packages that you will need during this assignment.\n",
    "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- [scipy](https://www.scipy.org) provides many numerical routines for scientific computing.\n",
    "- [sklearn](http://scikit-learn.org/stable/) provides simple and efficient tools for data analysis and machine learning.\n",
    "- [xgboost](https://github.com/dmlc/xgboost) is an highly-optimized library for gradient boosting.\n",
    "- [pandas](https://pandas.pydata.org) is a library providing easy-to-use data structures and data analysis tools.\n",
    "- [matplotlib](http://matplotlib.org) is a library for plotting graphs in Python.\n",
    "- [seaborn](https://seaborn.pydata.org/index.html) provides a high-level interface for drawing attractive statistical graphics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_val_predict\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV, ElasticNet, ElasticNetCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.base import clone\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset\n",
    "\n",
    "The Ames Housing dataset describes the sale of individual residential property in Ames, Iowa from 2006 to 2010. It is a modernized and expanded version of the often cited Boston Housing dataset. More information can be found [here](https://ww2.amstat.org/publications/jse/v19n3/decock.pdf).\n",
    "\n",
    "The Ames dataset contains 2930 observations and 80 variables (plus 2 observation identifiers) of different type: \n",
    " - 23 nominal\n",
    " - 23 ordinal\n",
    " - 14 discrete \n",
    " - 20 numerical.\n",
    " \n",
    "The goal is to build a regression model for the variable `SalePrice`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give a quick look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/AmesHousing.csv\", sep=\";\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the dataset needs some feature engeneering. Let's start with a simple task.\n",
    "\n",
    "---\n",
    "**Exercise:** Remove the columns `Order` and `PID` from the dataset.\n",
    "\n",
    "**Hint:** Use the pandas function `drop()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e343735003f35df24616a56cfd702054",
     "grade": false,
     "grade_id": "cell-f423b46978321414",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈ 1 line)\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset size: \" + str(data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "       \n",
    "<table style=\"width:20%\">\n",
    "  \n",
    "  <tr>\n",
    "    <td>**Dataset size**</td>\n",
    "    <td> (2930, 80) </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e0da1755a05928c3f19ef3ae28a4df05",
     "grade": true,
     "grade_id": "cell-9dc8bfcc1c773c1b",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert data.shape == (2930,80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Missing data \n",
    "\n",
    "Missing data occur when no data value is stored for a variable in an observation. This is a common occurrence in real-world datasets, and it can have a significant effect on the conclusions that can be drawn from the data. Understanding the reasons why data are missing is important to correctly handle the remaining data. \n",
    "\n",
    "The following code computes the amount of missing data for each variable in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing values\n",
    "total = data.isnull().sum().sort_values(ascending=False)\n",
    "\n",
    "# show\n",
    "pd.DataFrame(data={'Missing': total}).head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By reading the data description, we can understand that missing values actually have a precise meaning for some features:\n",
    " - **PoolQC**: NA means *No Pool*.\n",
    " - **MiscFeature**: NA means *No Misc Feature*.\n",
    " - **Alley**: : NA means *No Alley*.\n",
    " - **Fence**: : NA means *No Fence*.\n",
    " - **FireplaceQu** : NA means *No Fireplace*.\n",
    " - **GarageXXX**: NA means *No Garage*.\n",
    " - **BsmtXXX**: NA means *No Basement*.\n",
    " - **MasVnrXXX**: NA means *No masonry veneer*.\n",
    "\n",
    "For other features, missing data can be replaced with \"default\" values:\n",
    " - **LotFrontage** : Since the area of each street connected to the house property most likely have a similar area to other houses in its neighborhood , we can fill in missing values by the median `LotFrontage` of the neighborhood.\n",
    " - **Electrical** : It has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "**Exercise:** Replace missing data with the appropriate \"default\" values. \n",
    " \n",
    " - Use the string \"None\" in `PoolQC`, `MiscFeature`, `Alley`, `Fence`, `FireplaceQu`.\n",
    "     \n",
    " - Use the string \"None\" in `GarageType`, `GarageFinish`, `GarageQual`, `GarageCond`.\n",
    "  \n",
    " - Use the value 0 in `GarageYrBlt`, `GarageArea`, `GarageCars`.\n",
    " \n",
    " - Use the string \"None\" in `BsmtQual`, `BsmtCond`, `BsmtExposure`, `BsmtFinType1`, `BsmtFinType2`.\n",
    "     \n",
    " - Use the value 0 in `BsmtFinSF1`, `BsmtFinSF2`, `BsmtUnfSF`, `TotalBsmtSF`, `BsmtFullBath`, `BsmtHalfBath`.\n",
    " \n",
    " - Use the string \"None\" in `MasVnrType` and the value 0 in `MasVnrArea`.\n",
    " \n",
    " - Use the most common value in `Electrical`.\n",
    "   \n",
    "\n",
    "**Hint:** Use the pandas function `fillna()` to replace NA with a given value; use the pandas function `mode()` to find the most common value of a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "041a3225844864aeb296b8e339c5ff6e",
     "grade": false,
     "grade_id": "cell-38c465d9a2b6bec0",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "none_features = ('PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'GarageType', \n",
    "                 'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond', \n",
    "                 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType')\n",
    "zero_features = ('GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', \n",
    "                 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea')\n",
    "cst_feature = 'Electrical'\n",
    "\n",
    "### START CODE HERE ### (≈ 5 line)\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = data.isnull().sum().sort_values(ascending=False)\n",
    "pd.DataFrame(data={'Missing': total}).head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "       \n",
    "<table style=\"width:30%\">\n",
    "  \n",
    "  <tr>\n",
    "    <td>**LotFrontage**</td>\n",
    "    <td> 490 </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**SalePrice**</td>\n",
    "    <td> 0 </td> \n",
    "  </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2ecd16bf7fe769885fc934572bd417c2",
     "grade": true,
     "grade_id": "cell-82a94d914277deb2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "for col in (none_features + zero_features + (cst_feature,)):\n",
    "    assert data[col].isnull().sum() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Exercise:** Replace the missing data in `LotFrontage` with the following procedure.\n",
    " 1. Group the values in `LotFrontage` having the same `neighborhood`.\n",
    " - Compute the median value of `LotFrontage` on each group separately.\n",
    " - For each group, replace the missing values in `LotFrontage` with the corresponding median.\n",
    "  \n",
    "**Hint:** Use the pandas function `groupby()` to group observations by the value of a column, and the function `transform()` to manipulate each group separately. See this [tutorial](http://pbpython.com/pandas_transform.html) for more information about groups in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "38225bf9ff54a9aff98f75efa837fc58",
     "grade": false,
     "grade_id": "cell-e26d60bd039074f9",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈ 1 line)\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = data.isnull().sum().sort_values(ascending=False)\n",
    "pd.DataFrame(data={'Missing': total}).head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "       \n",
    "<table style=\"width:30%\">\n",
    "  \n",
    "  <tr>\n",
    "    <td>**LotFrontage**</td>\n",
    "    <td> 3 </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**SalePrice**</td>\n",
    "    <td> 0 </td> \n",
    "  </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e95bf9ad5884b6bffddc055c4fe48378",
     "grade": true,
     "grade_id": "cell-12ddf0150e7d4487",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert data['LotFrontage'].isnull().sum() == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the previous approach, we weren't able to determine `LotFrontage` for 3 houses, as all the observations in the corresponding neighborhoods have missing values in `LotFrontage`. To avoid the risk of introducing a bias in the data, we decide to drop these observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = data['Neighborhood'].isin([\"Landmrk\",\"GrnHill\"])\n",
    "data = data[~mask]\n",
    "\n",
    "print(\"Missing values: \" + str(data.isnull().sum().sum()))\n",
    "print(\"Dataset shape: \" + str(data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Correlation matrix\n",
    "\n",
    "Now, it's time to analyze the correlation between `SalePrice` and the other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix\n",
    "corrmat = data.corr()\n",
    "\n",
    "# variables for heatmap\n",
    "k = 10 \n",
    "cols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\n",
    "cm = corrmat[cols].loc[cols]\n",
    "\n",
    "# show\n",
    "sns.set(font_scale=1.5)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the top-10 correlation matrix, we can deduce that:\n",
    "\n",
    "- `OverallQual`, `GrLivArea` and `TotalBsmtSF` are strongly correlated with `SalePrice`.\n",
    "- `GarageCars` is correlated to `GarageArea`. This seems reasonable, as the number of cars fitting into a garage is proportional to the garage area.\n",
    "- `TotalBsmtSF` is correlated to `1stFlrSF`. This seems reasonable, as the floors of an house are usually the same size.\n",
    "\n",
    "Now, let's proceed to the scatter plots between `SalePrice` and the most correlated variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "\n",
    "cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'TotalBsmtSF']\n",
    "sns.pairplot(data[cols], size = 2.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables `OverallQual`, `GrLivArea` and `TotalBsmtSF` seem to be linearly related with `SalePrice`. The relationships are positive, which means that as one variable increases, the other also increases.\n",
    "\n",
    "Since area-related features are very important to determine house prices, we are going to craft a new feature that indicates the total area of basement and above ground living space.\n",
    "\n",
    "---\n",
    "**Exercise:** Add a new feature `TotalSF` given by the sum of `GrLivArea` and `TotalBsmtSF`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7a235a077e8ce26401d9eb81cc74e118",
     "grade": false,
     "grade_id": "cell-a1a66ff077900b9b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "new_feature = 'TotalSF'\n",
    "\n",
    "### START CODE HERE ### (≈ 1 line)\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset shape: \" + str(data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "       \n",
    "<table style=\"width:30%\">\n",
    "  \n",
    "  <tr>\n",
    "    <td>**Dataset shape**</td>\n",
    "    <td> (2927, 81) </td> \n",
    "  </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "05b5683c3091069bd123c375d3e45d71",
     "grade": true,
     "grade_id": "cell-6baeaa7f95d92c6d",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert data.shape == (2927, 81)\n",
    "assert 'TotalSF' in data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Outliers\n",
    "\n",
    "Outliers are something that we should be aware of. Why? Because outliers can markedly affect our models, and can be a valuable source of information, providing us insights about specific behaviours. Outlier handling is a complex subject. Here, we'll just do a quick analysis through the scatter plots.\n",
    "\n",
    "Let's plot `SalePrice` versus `GrLivArea`, so as to easily spot any large houses that sold for really cheap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data.GrLivArea, data.SalePrice)\n",
    "plt.title(\"Looking for outliers\")\n",
    "plt.xlabel(\"GrLivArea\")\n",
    "plt.ylabel(\"SalePrice\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what we can deduce from the scatterplot.\n",
    "\n",
    "- The three values with bigger `GrLivArea` seem strange and they are not following the crowd. Maybe they refer to agricultural area and that could explain the low price. Even though we are not sure about this, it seems that these points are not representative of the typical case. Therefore, we'll define them as outliers and delete them.\n",
    " \n",
    "- The two observations in the top of the plot (above 700'000) are also suspect. They look like two special cases, however they seem to be following the trend. For that reason, we will keep them.\n",
    "\n",
    "Outliers removal is not always safe. The points that we have decided to delete are very huge and really bad (extremely large areas for very low prices). There are probably others outliers in the training data. However, removing all of them may affect badly our models if ever there were also outliers in new data. That's why, instead of removing them all, we will just manage to make some of our models robust on them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Exercise:** Remove the outliers in which `SalePrice` is less than 300'000 and `GrLivArea` is greater than 4'000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b246c8ef890e544a91ff6d61fc56e075",
     "grade": false,
     "grade_id": "cell-a09829d30657d7dd",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈ 1 line)\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(data['GrLivArea'], data['SalePrice'])\n",
    "plt.ylabel('SalePrice', fontsize=13)\n",
    "plt.xlabel('GrLivArea', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset shape: \" + str(data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "       \n",
    "<table style=\"width:30%\">\n",
    "  \n",
    "  <tr>\n",
    "    <td>**Dataset shape**</td>\n",
    "    <td> (2924, 81) </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c1231590818df7638e39ec39c58504ed",
     "grade": true,
     "grade_id": "cell-678d939c3e60c7fe",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert data.shape == (2924, 81)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Categorical variables\n",
    "\n",
    "The dataset contains different types of variable.\n",
    " - **Nominal** - This variable can take a fixed number of values. \n",
    " - **Ordinal** - This variable can take a fixed number of values, and such values have a logical order.\n",
    " - **Discrete** - This variable can take any integer value.\n",
    " - **Numerical** - This variable can take any numerical value.\n",
    "\n",
    "Let's start by grouping the variables according to their type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal = ['MSSubClass', 'MSZoning', 'Alley', 'LandContour', 'LotConfig', 'Neighborhood', 'Condition1', 'Condition2', \n",
    "           'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Foundation',\n",
    "           'Heating', 'CentralAir', 'Electrical', 'GarageType', 'MiscFeature', 'MoSold', 'SaleType', 'SaleCondition'] \n",
    "           \n",
    "ordinal = ['LotShape', 'Utilities', 'LandSlope', 'OverallQual', 'OverallCond', 'ExterQual', 'ExterCond', 'BsmtQual', \n",
    "           'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'KitchenQual', 'Functional', \n",
    "           'FireplaceQu', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence']\n",
    "           \n",
    "discrete = ['YearBuilt', 'YearRemodAdd', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', \n",
    "            'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'YrSold']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordinal variables are converted into discrete features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ordinal:\n",
    "    v = list(data[col].values)\n",
    "    data[col] = LabelEncoder().fit_transform(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nominal variables are converted into binary features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast to string\n",
    "for col in nominal:\n",
    "    data[col] = data[col].apply(str)\n",
    "\n",
    "# convert to binary features\n",
    "data = pd.get_dummies(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Dataset split\n",
    "\n",
    "Finally, we can split the available data in train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(data['SalePrice'])\n",
    "X = np.array(data.drop('SalePrice', axis=1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Baseline models\n",
    "\n",
    "There exist many different approaches to deal with regression. In this notebook, we'll consider some of the models available in [scikit-learn](http://scikit-learn.org/stable/):\n",
    " - Linear regression\n",
    " - Regularized regression\n",
    " - Random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Linear regression\n",
    "\n",
    "Let's start with linear regression, which is the simplest model we could think of. The following code performs these steps:\n",
    " - The model is fit to the train set, \n",
    " - The performance is evaluated on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build\n",
    "linreg = LinearRegression()\n",
    "\n",
    "# train\n",
    "linreg.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred = linreg.predict(X_test)\n",
    "\n",
    "# test\n",
    "score = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "# show\n",
    "print('Linear regression: {:.2f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To gain insight into the learned model, we can visualize the residual plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.residplot(y_pred, y_test, lowess=True, color=\"r\", scatter_kws={\"color\":\"g\", \"s\":20, \"alpha\": 0.4})\n",
    "plt.title(\"Residual plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the residuals are **not** uniformely distributed around the zero (the red line is u-shaped). This is a clear indication that there is a **nonlinear** relationship between the target variable `SalePrice` and the other variables. To figure out what kind of nonlinearity it could be, we need to go deep and understand how `SalePrice` complies with the statistical assumptions that enable us to apply linear regression. The main assumption that we should test is **normality**. In this notebook, we'll just check univariate normality by looking at:\n",
    " - **Histogram**, which tells us about kurtosis and skewness.\n",
    " - **Normal probability plot**, in which data should closely follow the diagonal that represents the normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,4))\n",
    "fig.add_subplot(121)\n",
    "sns.distplot(data['SalePrice'], fit=norm);\n",
    "fig.add_subplot(122)\n",
    "res = stats.probplot(data['SalePrice'], plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the distribution of `SalePrice` deviates from the normal distribution and has positive skewness. A simple data transformation can solve the problem. This is one of the awesome things you can learn in statistical books: in case of positive skewness, a **logarithm** transformation usually works well. Here's how such a transformation works:\n",
    "\n",
    " - Train the model to predict `LogSalePrice = log(SalePrice + 1)`.\n",
    " \n",
    " - Then, revert to the actual prediction using the formula `y = exp(LogSalePrice) - 1`.\n",
    " \n",
    "---\n",
    "\n",
    "**Exercise:** Implement the log transformation of the target variable in such a way that can be applied to any regression model available in scikit-learn.\n",
    "\n",
    "**Hint:** You can implement a class that wraps a scikit-learn model, and redefines the following methods:\n",
    " - `fit(X, y)` - Transform the target variable `y` into `z = log(y+1)`, and then call `fit(X, z)` on the wrapped model.\n",
    " - `predict(X)` - Obtain the prediction `z` by calling `predict(X)` on the wrapped model, and then return `exp(z)-1`\n",
    " \n",
    "For numerical robustness, use the numpy functions `log1p` and `expm1` to perform the actual tranformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "227aa678de94222a8b2f21f23e8a866e",
     "grade": false,
     "grade_id": "cell-a99fd1f5b4815a9d",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class LogTransform:\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        ### START CODE HERE ###\n",
    "        None\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        ### START CODE HERE ###\n",
    "        None\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return np.sqrt(mean_squared_error(y, y_pred))\n",
    "    \n",
    "    def set_params(self, **kwargs):\n",
    "        self.model = kwargs['model']\n",
    "        return self\n",
    "        \n",
    "    def get_params(self, deep=True):\n",
    "        return {'model': self.model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build\n",
    "linreg = LogTransform(model=LinearRegression())\n",
    "\n",
    "# train\n",
    "linreg.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred = linreg.predict(X_test)\n",
    "\n",
    "# test\n",
    "score = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "# show\n",
    "print('Linear regression: {:.2f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table style=\"width:30%\">\n",
    "  \n",
    "  <tr>\n",
    "    <td>**Linear regression**</td>\n",
    "    <td> 19880.33 </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6f8f87fd8013ef520d77eef6724c5622",
     "grade": true,
     "grade_id": "cell-cb6a09dbea027a50",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(score, 19880.33, decimal=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's look again at the residual plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.residplot(y_pred, y_test, lowess=True, color=\"r\", scatter_kws={\"color\":\"g\", \"s\":20, \"alpha\": 0.4})\n",
    "plt.title(\"Residual plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the figure, we can see that the residuals are now regularly distributed around the zero. Hurrah!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Regularized regression\n",
    "\n",
    "Other approaches worth exploring are ridge regression, lasso, and elastic net. They are all linear regression models, differing from each other in the regularization term (used to reduce the risk of over-fitting).\n",
    " - **Ridge** - L2 regularization.\n",
    " - **Lasso** - L1 regularization.\n",
    " - **Elastic net** - Both.\n",
    "\n",
    "Regularization works best when the data are normalized. The code below uses the Scikit-Learn method `make_pipeline` to define a pipeline in which the data are properly normalized before training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = make_pipeline(RobustScaler(), LogTransform(model=Ridge(random_state=42)))\n",
    "lasso = make_pipeline(RobustScaler(), LogTransform(model=Lasso(random_state=42)))\n",
    "elnet = make_pipeline(RobustScaler(), LogTransform(model=ElasticNet(random_state=42)))\n",
    "\n",
    "models = (ridge, lasso, elnet)\n",
    "names  = ('Ridge', 'Lasso', 'Elnet')\n",
    "\n",
    "for clf, name in zip(models, names):\n",
    "    \n",
    "    # train\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # predict\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # test\n",
    "    score = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    # show\n",
    "    print('{:s}: {:.2f}'.format(name, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the performances of lasso and elastic net are not satisfactory. This happens because such models depend on hyper-parameters that control how much regularization is applied during training. In order to find the hyper-parameters leading to the best performance, you can use cross-validation. \n",
    "\n",
    "---\n",
    "\n",
    "**Exercise:** Find the best hyper-parameters via cross-validation. **You are only allowed to use the train set.**\n",
    " \n",
    "**Hint:** The following table reports the candidate values that you should test out with cross-validation. \n",
    " \n",
    " <table style=\"width:60%\"> \n",
    "  <tr>\n",
    "    <td></td>\n",
    "    <td>**Hyper-parameter**</td>\n",
    "    <td style=\"width:60%\">**Values**</td> \n",
    "  </tr> \n",
    "  <tr>\n",
    "    <td>**Ridge**</td>\n",
    "    <td>*alpha*</td>\n",
    "    <td>1, 5, 10, 15, 30, 50</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>**Lasso**</td>\n",
    "    <td>*alpha*</td>\n",
    "    <td>0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1</td> \n",
    "  </tr> \n",
    "  <tr>\n",
    "    <td>**Elastic net**</td>\n",
    "    <td>*alpha*</td>\n",
    "    <td>0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td></td>\n",
    "    <td>*l1_ratio*</td>\n",
    "    <td>0.2, 0.4, 0.6, 0.8, 1</td> \n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "To save time, you can adopt a coarse-to-fine approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1c641514f18a971c93d3fad08b8e6d61",
     "grade": false,
     "grade_id": "cell-4fa1e1037e4180b5",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ### \n",
    "ridge_alphas = (1,)  # TODO: Add candidate values\n",
    "lasso_alphas = (1,)\n",
    "elnet_alphas = (1,)\n",
    "elnet_l1     = (1,)\n",
    "### END CODE HERE ###\n",
    "\n",
    "\n",
    "# cross-validation strategy\n",
    "cv = KFold(n_splits=5, random_state=42)\n",
    "\n",
    "# models\n",
    "ridge_cv = RidgeCV(alphas=ridge_alphas, cv=cv)\n",
    "lasso_cv = LassoCV(alphas=lasso_alphas, cv=cv, random_state=42)\n",
    "elnet_cv = ElasticNetCV(alphas=elnet_alphas, l1_ratio=elnet_l1, max_iter=5000, cv=cv, random_state=42)\n",
    "\n",
    "# pipelines\n",
    "ridge = make_pipeline(RobustScaler(), LogTransform(model=ridge_cv))\n",
    "lasso = make_pipeline(RobustScaler(), LogTransform(model=lasso_cv))\n",
    "elnet = make_pipeline(RobustScaler(), LogTransform(model=elnet_cv))\n",
    "\n",
    "# train\n",
    "ridge.fit(X_train, y_train)\n",
    "lasso.fit(X_train, y_train)\n",
    "elnet.fit(X_train, y_train)\n",
    "    \n",
    "# print\n",
    "print('Ridge: {:.2f}'.format(ridge.score(X_test,y_test)))\n",
    "print('Lasso: {:.2f}'.format(lasso.score(X_test,y_test)))\n",
    "print('Elnet: {:.2f}'.format(elnet.score(X_test,y_test)))\n",
    "print('-----')\n",
    "print('Best hyper-params (ridge): alpha={:2.2f}'.format(ridge_cv.alpha_))\n",
    "print('Best hyper-params (lasso): alpha={:2.5f}'.format(lasso_cv.alpha_))\n",
    "print('Best hyper-params (elnet): alpha={:2.5f}, l1_ratio={:2.2f}'.format(elnet_cv.alpha_, elnet_cv.l1_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table style=\"width:30%\">\n",
    "  \n",
    "  <tr>\n",
    "    <td>**Ridge**</td>\n",
    "    <td> 18836.40 </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**Lasso**</td>\n",
    "    <td> 19044.91 </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**Elnet**</td>\n",
    "    <td> 18980.21 </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d6e6a3c266d7ad50c2fe5832b3a63bd8",
     "grade": true,
     "grade_id": "cell-6456f58e76ea4922",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(ridge.score(X_test,y_test), 18836.40, decimal=0)\n",
    "np.testing.assert_almost_equal(lasso.score(X_test,y_test), 19044.91, decimal=0)\n",
    "np.testing.assert_almost_equal(elnet.score(X_test,y_test), 18980.21, decimal=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's check the residual plot of elastic net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.residplot(elnet.predict(X_test), y_test, lowess=True, color=\"r\", scatter_kws={\"color\":\"g\", \"s\":20, \"alpha\": 0.4})\n",
    "plt.title(\"Residual plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Random forest\n",
    "\n",
    "In the realm of nonlinear models, random forest is one of the most interesting techniques. The idea behind it is to build several decision trees independently, and then to average their predictions. In particular, each tree is learned from bootstrapped observations, using a random subset of the features at each node split. As a result of this randomness, the combined trees yield a prediction model that is usually better than any of the individual tree.\n",
    "\n",
    "Random forest depends on the following hyper-parameters:\n",
    " - `n_estimators` - The number of trees in the forest.\n",
    " - `max_depth` - The maximum depth of each individual tree.\n",
    " - `max_features` - The number of features to consider when looking for the best split.\n",
    " - `min_samples_leaf` - The minimum number of samples required to be at a leaf node.\n",
    " \n",
    "---\n",
    "\n",
    "**Exercise:** Find the best hyper-parameters via cross-validation. **You are only allowed to use the train set.**\n",
    " \n",
    "**Hint:** The following table reports the candidate values that you should test out with cross-validation.\n",
    " \n",
    " <table style=\"width:60%\"> \n",
    "  <tr>\n",
    "    <td>**Hyper-parameter**</td>\n",
    "    <td style=\"width:60%\">**Values**</td> \n",
    "  </tr> \n",
    "  <tr>\n",
    "    <td>*n_estimators*</td>\n",
    "    <td>60, 70, 80, 90, 100</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>*max_features*</td>\n",
    "    <td>0.2, 0.4, 0.6, 0.8, 1</td> \n",
    "  </tr>\n",
    "   <tr>\n",
    "    <td>*max_depth*</td>\n",
    "    <td>None</td> \n",
    "  </tr> \n",
    "  <tr>\n",
    "    <td>*min_samples_leaf*</td>\n",
    "    <td>1</td> \n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "To save time, you can adopt a coarse-to-fine approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f3eb022236ec5439af9fff118ee591da",
     "grade": false,
     "grade_id": "cell-a59621b2db775327",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "forest = RandomForestRegressor(random_state=42)\n",
    "\n",
    "### START CODE HERE ### \n",
    "params_grid = { \n",
    "    \"n_estimators\": [10], # TODO: add candidate values\n",
    "                          # TODO: add the other parameter and its candidate values\n",
    "}\n",
    "### END CODE HERE ###\n",
    "\n",
    "\n",
    "# cross-validation strategy\n",
    "cv = KFold(n_splits=5, random_state=42)\n",
    "\n",
    "# hyper-parameter search\n",
    "forest_cv = GridSearchCV(forest, params_grid, scoring='neg_mean_squared_error', cv=cv, verbose=1, n_jobs=4)\n",
    "forest_cv.fit(X_train, y_train)\n",
    "\n",
    "# best score\n",
    "score = np.sqrt(-forest_cv.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Random forest: {:.2f}'.format(score))\n",
    "print('-----')\n",
    "print('Best hyper-params: ' + str(forest_cv.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table style=\"width:30%\">\n",
    "  \n",
    "  <tr>\n",
    "    <td>**Random forest**</td>\n",
    "    <td> 20740.51 </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**max_features**</td>\n",
    "    <td> 0.4 </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**n_estimators**</td>\n",
    "    <td> 80 </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6c574aea67c71c3c738e5b52fbf3f956",
     "grade": true,
     "grade_id": "cell-bb36718a72b4133b",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(score, 20740.51, decimal=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's check the residual plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.residplot(forest_cv.predict(X_test), y_test, lowess=True, color=\"r\", scatter_kws={\"color\":\"g\", \"s\":20, \"alpha\": 0.4})\n",
    "plt.title(\"Residual plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradient boosting\n",
    "\n",
    "Gradient boosting is one of the most powerful techniques for learning an ensemble of simple models (e.g., decision trees). In this notebook, we'll use the rich and fast implementation provided by [XGBoost](https://github.com/dmlc/xgboost). \n",
    "\n",
    "In order to get the best performance out of XGBoost, some important hyper-parameters need to be carefully tuned.\n",
    "\n",
    " 1. **Number of trees.** This parameter controls the ensemble size. In general, you want the number of trees to be as high as possible (>1000), but having it too high could lead to overfitting. It is usually set via early stopping.\n",
    " \n",
    " - **Learning rate.** This parameter scales the effect of each tree on the overall prediction. It is tightly coupled with the ensemble size: a smaller value of the learning rate will require a larger number of trees, and vice versa. Typically, a much better generalization error is obtained with small values (<0.1).\n",
    " \n",
    " - **Maximum tree depth**. This parameter controls the tree dept. Good values for it are generally between 1-10.\n",
    " \n",
    " - **Row & column subsampling**. These parameters allow to perform a random subsampling of observations and features (like random forests). Tipical values are between 0.5 and 1.\n",
    " \n",
    " - **L1 & L2 regularization**. These parameters allow to add a penalization on the weights used to combine the trees into the final prediction. Big values will make the model more conservative.\n",
    "\n",
    "There are many other hyper-parameters in XGBoost. You can refer to [this page](https://github.com/dmlc/xgboost/blob/master/doc/parameter.md) for the full list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 XGBRegressor\n",
    "\n",
    "In this notebook, we'll use the Scikit-Learn wrapper interface of the XGBoost regression model: `XGBRegressor`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "boost = xgb.XGBRegressor(seed=42)\n",
    "\n",
    "# train\n",
    "boost.fit(X_train, y_train)\n",
    "\n",
    "# test\n",
    "y_pred = boost.predict(X_test)\n",
    "score = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# show\n",
    "print('Gradient boosting: {:.2f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with the default hyper-parameters, gradient boosting performs on par with baseline models. Let's see if we can do better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Ensemble size\n",
    "\n",
    "The number of trees in the ensemble is one of the hyper-parameters that control the tradeoff between **under-fitting** and **over-fitting**. To determine if the ensemble is overfitting, we can keep track of the performance over the train and test sets during the learning phase. So doing, we'll be able to monitor how the prediction evolves as we add more trees to the ensemble. \n",
    "\n",
    "The XGBoost model can evaluate the performance on one or more datasets during training. This can be done by invoking the method `fit()` with two additional arguments: a list of datasets and the evaluation metric. Some of the [metrics](http://xgboost.readthedocs.io/en/latest//parameter.html#learning-task-parameters) supported by XGBoost are:\n",
    " - `rmse` for root mean squared error\n",
    " - `mae` for mean absolute error\n",
    " - `logloss` for binary logarithmic loss \n",
    " - `mlogloss` for multi-class cross-entropy loss\n",
    " - `error` for classification error\n",
    " - `auc` for area under ROC curve.\n",
    "\n",
    "The following code shows how to keep track of RMSE on both the train and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ensemble\n",
    "boost = xgb.XGBRegressor(n_estimators=500, learning_rate=0.5, seed=42)\n",
    "\n",
    "# Prepare the evaluation sets\n",
    "eval_set = [(X_train,y_train),(X_test,y_test)]\n",
    "\n",
    "# Train the ensemble\n",
    "boost.fit(X_train, y_train, eval_set=eval_set, eval_metric=\"rmse\", verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the train and test errors as a function of the number of trees. This is called **deviance plot**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deviance_plot(boost, ax=None, label='', train_color='#2c7bb6', test_color='#d7191c', alpha=1.0):\n",
    "    \n",
    "    results = boost.evals_result()\n",
    "    train_scores = results['validation_0']['rmse']\n",
    "    test_scores  = results['validation_1']['rmse']\n",
    "    n_estimators = boost.n_estimators\n",
    "\n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=(15,5))\n",
    "        ax = plt.gca()\n",
    "        \n",
    "    ax.plot(np.arange(n_estimators) + 1, train_scores, color=train_color,  label='Train %s' % label, linewidth=2, alpha=alpha)\n",
    "    ax.plot(np.arange(n_estimators) + 1, test_scores, color=test_color, label='Test %s' % label, linewidth=2, alpha=alpha)\n",
    "    ax.set_ylabel('Error')\n",
    "    ax.set_xlabel('Number of trees in the ensemble')\n",
    "    ax.legend(loc='upper right')\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = deviance_plot(boost)\n",
    "ax.set_ylim(0,40000)\n",
    "\n",
    "train_scores = boost.evals_result()['validation_0']['rmse']\n",
    "test_scores = boost.evals_result()['validation_1']['rmse']\n",
    "argmin = np.argmin(test_scores)\n",
    "minim = np.min(test_scores)\n",
    "\n",
    "unidir_arrow = {'xycoords': 'data', 'textcoords': 'data', 'arrowprops': {'arrowstyle': '->'}}\n",
    "bidir_arrow  = {'xycoords': 'data', 'textcoords': 'data', 'arrowprops': {'arrowstyle': '<->'}}\n",
    "ax.annotate('Lowest test error', xy=(argmin+1, minim), xytext=(argmin-20, minim+5000), **unidir_arrow)\n",
    "n_est = boost.n_estimators - 50\n",
    "ann = ax.annotate('', xy=(n_est, test_scores[n_est]), xytext=(n_est, train_scores[n_est]), **bidir_arrow)\n",
    "ax.text(n_est+2, (test_scores[n_est+1] + train_scores[n_est+1])/2, 'Overfitting')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that there is a **huge gap** between the train and the test errors. This is a clear sign that the model is **overfitting**. Indeed, after you add many trees to the ensemble, the model gets too much capacity (i.e., high variance) and starts fitting the idiosyncracies of the training data.\n",
    "\n",
    "Fortunately, XGBoost provides some knobs to reduce overfitting:\n",
    " - Early stopping\n",
    " - Tree-depth control\n",
    " - Random subsampling\n",
    " - Regularization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Early stopping\n",
    "\n",
    "One of the possible ways to fight overfitting is **early stopping**. The idea behind it is to stop the training when the model begins to overfit. In practice, the training can be stopped when no improvement is observed on the test error after a fixed number of epochs. \n",
    "\n",
    "To activate the early stopping in XGBoost, you need to pass three arguments to the method `fit()`: \n",
    " - `eval_set` - A list containing the test set (in the **last** position)\n",
    " - `eval_metric` - The performance metric\n",
    " - `early_stopping_rounds` - The number of epochs over which no improvement must be observed in order to stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "boost.fit(X_train, y_train, early_stopping_rounds=10, eval_set=eval_set, eval_metric=\"rmse\", verbose=False)\n",
    "\n",
    "# print\n",
    "print(\"Best score: {:2.2f}\".format(boost.best_score))\n",
    "print(\"Best tree limit: \" + str(boost.best_ntree_limit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Tree depth\n",
    "\n",
    "The hyper-paremeter `max_depth` controls the depth of each individual tree in the ensemble. A big value of `max_depth` increases the variance (i.e., the model tends to over-fit), whereas a small value increases the bias (i.e., the model tends to under-fit).\n",
    "\n",
    "The following plot illustrates the effect of this parameter on the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost_ctrl = xgb.XGBRegressor(n_estimators=500, max_depth=1, learning_rate=0.5, seed=42)\n",
    "\n",
    "# train\n",
    "boost.fit(X_train, y_train, eval_set=eval_set, eval_metric=\"rmse\", verbose=False)\n",
    "boost_ctrl.fit(X_train, y_train, eval_set=eval_set, eval_metric=\"rmse\", verbose=False)\n",
    "\n",
    "# show\n",
    "ax = deviance_plot(boost)\n",
    "ax = deviance_plot(boost_ctrl, ax=ax, label='(max_depth=1)', train_color='#abd9e9', test_color='#fdae61')\n",
    "ax.set_ylim(0,40000)\n",
    "\n",
    "n_est = boost.n_estimators - 100\n",
    "ax.annotate('Higher bias', xy=(n_est, boost_ctrl.evals_result()['validation_0']['rmse'][n_est]), xytext=(n_est-40, 5000), **unidir_arrow)\n",
    "ax.annotate('Lower variance', xy=(n_est-50, boost_ctrl.evals_result()['validation_1']['rmse'][n_est]), xytext=(n_est-100, 10000), **unidir_arrow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Learning rate\n",
    "\n",
    "The hyper-parameter `learning_rate` scales the effect of each tree on the overall prediction. It is tightly coupled with the ensemble size (i.e., the hyper-parameter `n_estimators`). A smaller value of `learning_rate` allows the ensemble to generalize better to new unseen data, but it requires a higher value of `n_estimators` to get good results. However, if the latter is too high, the model starts to overfit.\n",
    "\n",
    "The following plot illustrates the effect of this parameter on the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost_ctrl = xgb.XGBRegressor(n_estimators=500, learning_rate=0.1, seed=42)\n",
    "\n",
    "# train\n",
    "boost_ctrl.fit(X_train, y_train, eval_set=eval_set, eval_metric=\"rmse\", verbose=False)\n",
    "\n",
    "# show\n",
    "ax = deviance_plot(boost)\n",
    "ax = deviance_plot(boost_ctrl, ax=ax, label='(learning_rate=0.1)', train_color='#abd9e9', test_color='#fdae61')\n",
    "ax.set_ylim(0,40000)\n",
    "\n",
    "n_est = boost.n_estimators - 100\n",
    "ax.annotate('Higher bias', xy=(n_est, boost_ctrl.evals_result()['validation_0']['rmse'][n_est]), xytext=(n_est-40, 4000), **unidir_arrow)\n",
    "ax.annotate('Lower variance', xy=(n_est-30, boost_ctrl.evals_result()['validation_1']['rmse'][n_est]), xytext=(n_est-80, 14000), **unidir_arrow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Random subsampling\n",
    "\n",
    "The training of each individual tree in the ensemble can be controlled by three hyper-parameters:\n",
    " - `subsample` - Randomly collect a fraction of observations before growing each tree\n",
    " - `colsample_bytree` - Randomly select a fraction of variables before growing each tree\n",
    " - `colsample_bylevel` - Randomly select a fraction of variables before finding the best split node\n",
    "   \n",
    "Note that `subsample` and `learning_rate` interact with each other. When you set `subsample`, you should also lower `learning_rate` and increase `n_estimators`. Likewise, `colsample` interacts with `max_depth`. As a rule of thumb, a good value of `colsample` could be the square root of the total number of features. Also, keep in mind that feature sampling works better if there is a sufficient large number of features.\n",
    "\n",
    "The following plot illustrates the effect of these parameters on the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost = xgb.XGBRegressor(n_estimators=500, learning_rate=0.1, seed=42)\n",
    "boost_ctrl = xgb.XGBRegressor(n_estimators=500, colsample_bylevel=0.1, learning_rate=0.1, seed=42)\n",
    "\n",
    "# train\n",
    "boost.fit(X_train, y_train, eval_set=eval_set, eval_metric=\"rmse\", verbose=False)\n",
    "boost_ctrl.fit(X_train, y_train, eval_set=eval_set, eval_metric=\"rmse\", verbose=False)\n",
    "\n",
    "# show\n",
    "ax = deviance_plot(boost)\n",
    "ax = deviance_plot(boost_ctrl, ax=ax, label='(sampling=0.1)', train_color='#abd9e9', test_color='#fdae61')\n",
    "ax.set_ylim(0,40000)\n",
    "\n",
    "n_est = boost.n_estimators - 100\n",
    "ax.annotate('Higher bias', xy=(n_est, boost_ctrl.evals_result()['validation_0']['rmse'][n_est]), xytext=(n_est-40, 4000), **unidir_arrow)\n",
    "ax.annotate('Lower variance', xy=(n_est-30, boost_ctrl.evals_result()['validation_1']['rmse'][n_est]), xytext=(n_est-80, 14000), **unidir_arrow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Regularization\n",
    "\n",
    "The weights used to combine the trees into the final prediction can controlled by two hyper-parameters:\n",
    " - reg_lambda - Add a L2 regularization\n",
    " - reg_alpha - Add a L1 regularization \n",
    " \n",
    "The following plot illustrates the effect of these parameters on the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost = xgb.XGBRegressor(n_estimators=500, learning_rate=0.5, reg_lambda=0, seed=42)\n",
    "boost_ctrl = xgb.XGBRegressor(n_estimators=500, learning_rate=0.5, reg_lambda=10, seed=42)\n",
    "\n",
    "# train\n",
    "boost.fit(X_train, y_train, eval_set=eval_set, eval_metric=\"rmse\", verbose=False)\n",
    "boost_ctrl.fit(X_train, y_train, eval_set=eval_set, eval_metric=\"rmse\", verbose=False)\n",
    "\n",
    "# show\n",
    "ax = deviance_plot(boost, label='(lambda=0)')\n",
    "ax = deviance_plot(boost_ctrl, ax=ax, label='(lambda=10)', train_color='#abd9e9', test_color='#fdae61')\n",
    "ax.set_ylim(0,40000)\n",
    "\n",
    "n_est = boost.n_estimators - 100\n",
    "ax.annotate('Lower variance', xy=(n_est, boost_ctrl.evals_result()['validation_1']['rmse'][n_est]), xytext=(n_est-80, 14000), **unidir_arrow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Hyper-parameter tuning\n",
    "\n",
    "\n",
    "To obtain the best performance, you have to set hyper-parameters via cross-validation. Here is a good recipe:\n",
    " 1. Pick `n_estimators` as large as computationally possible\n",
    " - Activate early stopping on the test set\n",
    " - Use cross-validation to find the best `max_depth`, `learning_rate`, `subsample`, `colsample`, and `reg_lambda`\n",
    " - Retrain with the best hyper-parameters and early stopping.\n",
    " \n",
    "--- \n",
    "\n",
    "**Exercise:** Find the best-hyperparameters via cross-validation. **You are only allowed to use the train set.**\n",
    " \n",
    "**Hint:**\n",
    " - You may find useful the Scikit-Learn class `GridSearchCV`.\n",
    " - The following table reports the values that are typically tested out with cross-validation \n",
    " \n",
    " <table style=\"width:25%\"> \n",
    "  <tr>\n",
    "    <td>**Hyper-parameter**</td>\n",
    "    <td>**Values**</td> \n",
    "  </tr> \n",
    "  <tr>\n",
    "    <td>*n_estimators*</td>\n",
    "    <td>1000</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>*learning_rate*</td>\n",
    "    <td>0.001, 0.01, 0.1</td> \n",
    "  </tr> \n",
    "  <tr>\n",
    "    <td>*max_depth*</td>\n",
    "    <td>1, 2, 3, 4, 5</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>*subsample*</td>\n",
    "    <td>0.2, 0.4, 0.6, 0.8, 1</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>*colsample*</td>\n",
    "    <td>0.2, 0.4, 0.6, 0.8, 1</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>*reg_lambda*</td>\n",
    "    <td>0.1, 1, 10</td> \n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "- To save time, you can adopt a coarse-to-fine approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "026c00111617a3655265763d8c3e61f6",
     "grade": false,
     "grade_id": "cell-dc360f382fef9978",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# early stopping\n",
    "fit_params = {\"early_stopping_rounds\": 10, \n",
    "              \"eval_metric\": \"rmse\", \n",
    "              \"eval_set\": [(X_test, y_test)],\n",
    "              \"verbose\": False}\n",
    "\n",
    "### START CODE HERE ###\n",
    "# grid of parameters - a dictionary with entries: \"parameter name\": [value1, values2, ...]. \n",
    "paramGrid = { }\n",
    "### END CODE HERE ###\n",
    "\n",
    "\n",
    "# ensemble definition\n",
    "boost = xgb.XGBRegressor(n_estimators=1000)\n",
    "\n",
    "# cross-validation strategy\n",
    "cv = KFold(n_splits=5, random_state=42)\n",
    "\n",
    "# hyper-parameter search\n",
    "gridsearch = GridSearchCV(boost, paramGrid, cv=cv, verbose=2, n_jobs=4)\n",
    "gridsearch.fit(X_train, y_train, **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best hyperparameters: ' + str(gridsearch.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table style=\"width:30%\">\n",
    "  \n",
    "  <tr>\n",
    "    <td>**learning_rate**</td>\n",
    "    <td> 0.01 </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**max_depth**</td>\n",
    "    <td> 3 </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**colsample_bytree**</td>\n",
    "    <td> 0.4 </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**subsample**</td>\n",
    "    <td> 0.5 </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**reg_lambda**</td>\n",
    "    <td> 1 </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ec148024e00b4162086a499edd2ec104",
     "grade": true,
     "grade_id": "cell-5a9784443b93cc24",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert gridsearch.best_params_['learning_rate'] == 0.01\n",
    "assert gridsearch.best_params_['max_depth'] == 3\n",
    "assert gridsearch.best_params_['colsample_bytree'] == 0.4\n",
    "assert gridsearch.best_params_['subsample'] == 0.5\n",
    "assert gridsearch.best_params_['reg_lambda'] == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "You can now retrain the ensemble using the best hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "boost = xgb.XGBRegressor(n_estimators=1000, **gridsearch.best_params_)\n",
    "\n",
    "# train\n",
    "boost.fit(X_train, y_train, **fit_params)\n",
    "\n",
    "# test\n",
    "y_pred = boost.predict(X_test, ntree_limit=boost.best_ntree_limit)\n",
    "\n",
    "# score\n",
    "score = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# print\n",
    "print('Gradient boosting: {:.2f} (with {:d} trees)'.format(score, boost.best_ntree_limit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also check the residual plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.residplot(y_pred, y_test, lowess=True, color=\"r\", scatter_kws={\"color\":\"g\", \"s\":20, \"alpha\": 0.4})\n",
    "plt.title(\"Residual plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Stacking\n",
    "\n",
    "Stacked generalization is a way of combining multiple models through the concept of a meta learner. The general procedure is as follows:\n",
    "- **Base models.** Several models are trained to generate an intermediate set of features (e.g., their predictions).\n",
    "- **Meta learner.** A separate model is then trained to combine the intermediate features into the final prediction.\n",
    "\n",
    "The tricky part is figuring out how to train the meta learner. Here's how it could be done.\n",
    "1. Split the train set into two disjoint sets.\n",
    "2. Train the base models on the first part.\n",
    "3. Use them to generate the intermediate predictions on the second part.\n",
    "4. Train the meta-learner using the intermediate predictions as the inputs, and the correct responses as the outputs.\n",
    "\n",
    "The above procedure is practically the same as **hold-out** cross-validation, but it is also possible to use the **K-fold** approach (more on this later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Stacked average\n",
    "\n",
    "To gain more insight into the matter, let's begin with a simplyfied form of stacking, where the meta learner is just the average of the intermediate predictions, which has the advantage of requiring no training at all. The following code illustrates how to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base models\n",
    "models = [\n",
    "    LogTransform(model=LinearRegression()),\n",
    "    make_pipeline(RobustScaler(), LogTransform(Ridge(alpha=20))),\n",
    "    make_pipeline(RobustScaler(), LogTransform(Lasso(alpha=0.0005, random_state=42))),\n",
    "    make_pipeline(RobustScaler(), LogTransform(ElasticNet(alpha=0.001, l1_ratio=0.40, random_state=42))),\n",
    "    RandomForestRegressor(n_estimators=80, max_features=0.4, random_state=42),\n",
    "    xgb.XGBRegressor(n_estimators=1000, learning_rate=0.01, max_depth=3, subsample=0.5, colsample_bytree=0.4, seed=42)\n",
    "]\n",
    "\n",
    "y_stacked = np.zeros([len(y_test), len(models)+1])\n",
    "\n",
    "# intermediate predictions\n",
    "for i,m in enumerate(models):\n",
    "    \n",
    "    # train\n",
    "    m.fit(X_train, y_train)\n",
    "    \n",
    "    # test\n",
    "    y_stacked[:,i] = m.predict(X_test)\n",
    "\n",
    "# final prediction\n",
    "y_stacked[:,-1] = np.mean(y_stacked[:,0:-1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ('Linear regression', 'Ridge regression', 'Lasso regression', 'Elastic net', 'Random forest', 'Boosting', 'Stacked average')\n",
    "\n",
    "for i, name in enumerate(names):\n",
    "    score = np.sqrt(mean_squared_error(y_test, y_stacked[:,i]))\n",
    "    print( \"{:<20}{:.2f}\".format(name, score) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that even a simple average performs better than the base models. Now, let's try the actual stacking. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Hold-out stacking\n",
    "\n",
    "This variant of stacking works as follow:\n",
    " - Set aside some of the training data, and train the base models on the remaining part. \n",
    " - After the base models are trained, use them to generate the intermediate features from the hold-out data.\n",
    " - Train the meta learner using the intermediate features as inputs, and the hold-out labels as outputs.\n",
    " - On new data, use the base models to generate the intermediate predictions, and feed them to the meta learner.\n",
    " \n",
    "---\n",
    "\n",
    "**Exercise:** Implement stacking using the hold-out approach.\n",
    "\n",
    "**Hint:** You can implement a class wrapping the base models and the meta learner into a single ensemble. The class has the following attributes and methods:\n",
    " - `base_models` - a list of scikit-learn models.\n",
    " - `meta_learner` - a single scikit-learn model.\n",
    " - `fit(X, y)` - Train the base models and the meta learner (see above).\n",
    " - `predict(X)` - Predict on new data (see above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1db9e2cc49224d1fa0b2297135d360d9",
     "grade": false,
     "grade_id": "cell-4b235c1525a11417",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class HoldOutStacking:\n",
    "    \n",
    "    def __init__(self, base_models, meta_learner):\n",
    "        self.base_models = base_models\n",
    "        self.meta_learner = meta_learner\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # 1. Split the data\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        ### YOUR CODE HERE ###\n",
    "        \n",
    "        # 2. Train the base models\n",
    "        None\n",
    "        \n",
    "        # 3. Train the meta learner\n",
    "        None\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        # 1. Generate the intermediate features\n",
    "        None\n",
    "        \n",
    "        # 2. Return the final predictions\n",
    "        None\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking\n",
    "stack = HoldOutStacking(models, meta_learner=LassoCV())\n",
    "\n",
    "# train\n",
    "stack.fit(X_train, y_train)\n",
    "\n",
    "# test\n",
    "y_pred = stack.predict(X_test)\n",
    "\n",
    "# evaluate\n",
    "score = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# show\n",
    "print('Hold-out stacking: {:2.2f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table style=\"width:30%\">\n",
    "  \n",
    "  <tr>\n",
    "    <td>**Hold-out stacking**</td>\n",
    "    <td> 17867.76 </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "afb5f68280cc6fa6dd6ea9969ac59825",
     "grade": true,
     "grade_id": "cell-5425f03f13ecd3d8",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(score, 17867.76, decimal=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "You can see that hold-out stacking is not better than stacked average. The reason is that the base models are trained on a smaller train set, as a portion of the original training data is set aside for the meta learner. Fortunately, this issue can be solved by using the K-fold approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 K-fold stacking\n",
    "\n",
    "This variant of stacking works as follow:\n",
    " - Divide the training data into K different folds. \n",
    " - Train the base models using K-1 folds, and use them to make intermediate predictions from the unused fold. \n",
    " - Repeat the process by holding out a different fold, and continue until you have generated the intermediate predictions for all the K folds.\n",
    " - Use all the training data to train the base models, and all the intermediate predictions to train the meta learner.\n",
    " - On new data, use the base models to generate the intermediate predictions, and feed them to the meta learner.\n",
    " \n",
    "---\n",
    "\n",
    "**Exercise:** Implement stacking using the K-fold approach.\n",
    "\n",
    "**Hint:** You can implement a class wrapping the base models and the meta learner into a single ensemble. The class has the following attributes and methods:\n",
    " - `base_models` - a list of scikit-learn models.\n",
    " - `meta_learner` - a single scikit-learn model.\n",
    " - `fit(X, y)` - Train the base models and the meta learner (see above).\n",
    " - `predict(X)` - Predict on new data (see above).\n",
    " \n",
    "You can use the scikit-learn function `cross_val_predict()` to generate the intermediate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0951525a2ec09443d5c14b30c0220353",
     "grade": false,
     "grade_id": "cell-a0e0a3519d99bced",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class KFoldStacking:\n",
    "    \n",
    "    def __init__(self, base_models, meta_learner):\n",
    "        self.base_models = base_models\n",
    "        self.meta_learner = meta_learner\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # 1. Define the K-folds\n",
    "        cv = KFold(n_splits=5, random_state=42)\n",
    "        \n",
    "        ### YOUR CODE HERE ###\n",
    "        \n",
    "        # 2. Train the base models\n",
    "        None\n",
    "        \n",
    "        # 3. Train the meta learner\n",
    "        None\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        \n",
    "        # 1. Generate the intermediate features\n",
    "        X_meta = np.column_stack([model.predict(X) for model in self.base_models])\n",
    "        \n",
    "        # 2. Return the final predictions\n",
    "        return self.meta_learner.predict(X_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking\n",
    "stack = KFoldStacking(models, meta_learner=LinearRegression())\n",
    "\n",
    "# train\n",
    "stack.fit(X_train, y_train)\n",
    "\n",
    "# test\n",
    "y_pred = stack.predict(X_test)\n",
    "\n",
    "# evaluate\n",
    "score = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# show\n",
    "print('K-fold stacking: {:2.2f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table style=\"width:30%\">\n",
    "  \n",
    "  <tr>\n",
    "    <td>**K-fold stacking**</td>\n",
    "    <td> 17758.86 </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ffbdce424d49c6b3a50fecbfc91ab2f3",
     "grade": true,
     "grade_id": "cell-fc54c35e2112a463",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(score, 17758.86, decimal=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "You can see that K-fold stacking is slightly better than stacked average. However, there is still one thing that can be improved in our implementation. Remark that we train K different versions of the same base model to generate the intermediate features. This is how K-fold cross-validation works. However, we throw them away in the current implementation. Instead, we could make a better use of those models. Let's see how to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Power stacking\n",
    "\n",
    "This variant of stacking works as follow:\n",
    " - Divide the training data into K different folds. \n",
    " - Train the base models using K-1 folds, and use them to make intermediate predictions on the unused fold. \n",
    " - Repeat the process by holding out a different fold, and continue until you have ended up training K different versions of each base model, each of which generates the intermediate predictions for one of the K folds. \n",
    " - Use the intermediate predictions to train the meta learner.\n",
    " - On new data, use the K ensembles of base models to generate K different sets of intermediate predictions, and average them together to get a single set, and feed it to the meta learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PowerStacking:\n",
    "    \n",
    "    def __init__(self, base_models, meta_learner):\n",
    "        self.base_models = base_models\n",
    "        self.meta_learner = meta_learner\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # 1. Initialize\n",
    "        cv = KFold(n_splits=5, random_state=42)\n",
    "        models = self.base_models\n",
    "        self.base_models = [list() for x in models]\n",
    "        \n",
    "        # 2. Train the base models\n",
    "        X_meta = np.zeros((X.shape[0], len(models)))\n",
    "        for i, model in enumerate(models):\n",
    "            for train_idx, valid_idx in cv.split(X, y):\n",
    "                instance = clone(model)\n",
    "                self.base_models[i].append(instance)\n",
    "                instance.fit(X[train_idx,:], y[train_idx])\n",
    "                X_meta[valid_idx, i] = instance.predict(X[valid_idx,:])\n",
    "        \n",
    "        # 3. Train the meta learner\n",
    "        self.meta_learner.fit(X_meta, y)\n",
    "\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \n",
    "        # 1. Generate the intermediate features\n",
    "        X_meta = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, base_models in enumerate(self.base_models):\n",
    "            y_stacked = np.column_stack([model.predict(X) for model in base_models])\n",
    "            X_meta[:,i] = np.mean(y_stacked, axis=1)\n",
    "                    \n",
    "        # 2. Return the final predictions\n",
    "        return self.meta_learner.predict(X_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking\n",
    "stack = PowerStacking(models, meta_learner=LinearRegression())\n",
    "\n",
    "# train\n",
    "stack.fit(X_train, y_train)\n",
    "\n",
    "# test\n",
    "y_pred = stack.predict(X_test)\n",
    "\n",
    "# evaluate\n",
    "score = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# show\n",
    "print('Power stacking: {:2.2f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the performance has slightly improved. Let's also check the residual plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.residplot(y_pred, y_test, lowess=True, color=\"r\", scatter_kws={\"color\":\"g\", \"s\":20, \"alpha\": 0.4})\n",
    "plt.title(\"Residual plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Conclusion\n",
    "\n",
    "To recap, these are the performances obtained with the tested models. \n",
    "\n",
    "<table style=\"width:25%\">\n",
    "\n",
    "    <tr>\n",
    "    <td>**Linear regression**</td>\n",
    "    <td> 19880.33 </td> \n",
    "    </tr>\n",
    "    \n",
    "    <tr>\n",
    "    <td>**Ridge regression**</td>\n",
    "    <td> 18836.40 </td> \n",
    "    </tr>\n",
    "    \n",
    "    <tr>\n",
    "    <td>**Lasso regression**</td>\n",
    "    <td> 19044.91 </td> \n",
    "    </tr>\n",
    "    \n",
    "    <tr>\n",
    "    <td>**Elastic net**</td>\n",
    "    <td> 18980.21 </td> \n",
    "    </tr>\n",
    "    \n",
    "    <tr>\n",
    "    <td>**Random forest**</td>\n",
    "    <td> 20740.51 </td> \n",
    "    </tr>\n",
    "    \n",
    "    <tr>\n",
    "    <td>**Boosting**</td>\n",
    "    <td> 18512.43 </td> \n",
    "    </tr>\n",
    "    \n",
    "    <tr>\n",
    "    <td>**Stacked average**</td>\n",
    "    <td> 17835.13 </td> \n",
    "    </tr>\n",
    "    \n",
    "    <tr>\n",
    "    <td>**Hold-out stacking**</td>\n",
    "    <td> 17867.76 </td> \n",
    "    </tr>\n",
    "    \n",
    "    <tr>\n",
    "    <td>**K-fold stacking**</td>\n",
    "    <td> 17758.86 </td> \n",
    "    </tr>\n",
    "    \n",
    "    <tr>\n",
    "    <td>**Power stacking**</td>\n",
    "    <td> 17720.59 </td> \n",
    "    </tr>\n",
    "  \n",
    "</table>\n",
    "\n",
    "We can see that stacking achieves the best performance on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
