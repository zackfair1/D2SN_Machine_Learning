}
load("data2")
summary(df)
plot( df$x1, df$x2, pch=21, bg=c("red","blue")[df$y+1] )
svm = ksvm( y ~ x2 + x1, data=df, type="C-svc", kernel='rbfdot' )
svm
showSVM(svm, df)
svm2 = ksvm( y ~ x2 + x1, data=df, type="C-svc", kernel='vanilladot', C=lambda, scaled=FALSE )
svm2
showSVM(svm2, df)
showSVM(svm, df)
svm = ksvm( y ~ x2 + x1, data=df, type="C-svc", kernel='vanilladot' )
svm
showSVM(svm, df)
plot(svm, data=df)
lambda = 1;
svm2 = ksvm( y ~ x2 + x1, data=df, type="C-svc", kernel='vanilladot', C=lambda, scaled=FALSE )
svm2
showSVM(svm2, df)
# ---
#
# ### Question 1
# Your task is to try different values of **C** on this dataset.
#  - Can you set **C** such that all samples are correctly classified?
#  - How does the margin change by increasing/reducing **C**?
#  - In your opinion, should the value of **C** be small or large in order to have a classifier that performs well on new data?
#
seq(1,30)
# --------- Answer1 -----------
# seq(0.1,1,0.1)
# To visualize all the subplots
par(mfrow=c(3,2))
for (i in c(0.2,0.5,1,3,6,10,20,30)){
svm3 = ksvm( y ~ x2 + x1, data=df, type="C-svc", kernel='vanilladot', C=i, scaled=F )
print(svm3)
showSVM(svm3, df, i)
}
load("data2")
summary(df)
plot( df$x1, df$x2, pch=21, bg=c("red","blue")[df$y+1] )
svm = ksvm( y ~ x2 + x1, data=df, type="C-svc", kernel='rbfdot' )
svm
showSVM(svm, df)
setwd(dir = 'C:/Users/zacke/OneDrive - vgytk/D2SN/ML2/SVM - TP/R')
load("data1")
summary(df)
plot( df$x1, df$x2, pch=21, bg=c("red","blue")[df$y+1] )
library(kernlab)
svm = ksvm( y ~ x2 + x1, data=df, type="C-svc", kernel='vanilladot' )
svm
showSVM <- function(svm, frame, margin=TRUE, support=TRUE)
{
# create a grid of points
hs <- 0.01
x1_min <- min( frame[,1]) - 0.01
x1_max <- max( frame[,1]) + 0.01
x2_min <- min( frame[,2]) - 0.01
x2_max <- max( frame[,2]) + 0.01
grid <- as.matrix(expand.grid( seq(x1_min, x1_max, by = hs), seq(x2_min, x2_max, by =hs) ))
grid <- data.frame( x1 = grid[,1], x2 = grid[,2] )
# predict with the SVM
y = predict(svm, newdata = grid, type="decision")
# visualize points
plot( frame[,1], frame[,2], pch=21, cex=0.8, bg=c("red","blue")[frame[,3]+1] )
# highlight the support vectors
idx = unlist( alphaindex(svm) )
if(support)
points( frame[idx,1], frame[idx,2], col=c("red","blue")[frame[idx,3]+1], pch=5, cex=1.5)
# visualize the decision hyperplane
x1 = seq(x1_min,x1_max,by=hs)
x2 = seq(x2_min,x2_max,by=hs)
y <- matrix(y, nrow = length(x1), byrow = FALSE)
contour( x1, x2, y, levels=0, lwd=3, lty=1, drawlabels = FALSE, add=TRUE )
# visualize the margins
if(margin)
contour( x1, x2, y, levels=c(-1,1), lwd = 1, lty=2, drawlabels = FALSE, add=TRUE )
}
showSVM(svm, df)
plot(svm, data=df)
lambda = 1;
lambda = 3;
svm2 = ksvm( y ~ x2 + x1, data=df, type="C-svc", kernel='vanilladot', C=lambda, scaled=FALSE )
svm2
showSVM(svm2, df)
# ---
#
# ### Question 1
# Your task is to try different values of **C** on this dataset.
#  - Can you set **C** such that all samples are correctly classified?
#  - How does the margin change by increasing/reducing **C**?
#  - In your opinion, should the value of **C** be small or large in order to have a classifier that performs well on new data?
#
seq(1,30)
for (i in c(0.2,0.5,1,3,6,10,20,30)){
svm3 = ksvm( y ~ x2 + x1, data=df, type="C-svc", kernel='vanilladot', C=i, scaled=F )
print(svm3)
showSVM(svm3, df, i)
}
sig = 10
svm = ksvm( y ~ x2 + x1, data=df, type="C-svc", kernel='rbfdot', kpar=list(sigma=sig) )
svm
showSVM(svm, df)
load("data2")
summary(df)
plot( df$x1, df$x2, pch=21, bg=c("red","blue")[df$y+1] )
svm = ksvm( y ~ x2 + x1, data=df, type="C-svc", kernel='rbfdot' )
svm
showSVM(svm, df)
sig = 10
svm = ksvm( y ~ x2 + x1, data=df, type="C-svc", kernel='rbfdot', kpar=list(sigma=sig) )
svm
showSVM(svm, df)
sig = 30
svm = ksvm( y ~ x2 + x1, data=df, type="C-svc", kernel='rbfdot', kpar=list(sigma=sig) )
svm
showSVM(svm, df)
sig = 10
svm = ksvm( y ~ x2 + x1, data=df, type="C-svc", kernel='rbfdot', kpar=list(sigma=sig) )
svm
showSVM(svm, df)
for (i in c(0.2,0.5,1,3,6,10,15)){
svm3 = ksvm( y ~ x2 + x1, data=df, type="C-svc", kernel='rbfdot', kpar=list(sigma=i) )
print(svm3)
showSVM(svm3, df, i)
}
for (i in c(0.2,0.5,1,3,6,10,15,20)){
svm3 = ksvm( y ~ x2 + x1, data=df, type="C-svc", kernel='rbfdot', kpar=list(sigma=i) )
print(svm3)
showSVM(svm3, df, i)
}
for (i in c(0.2,0.5,1,3,6,10,15)){
svm3 = ksvm( y ~ x2 + x1, data=df, type="C-svc", kernel='rbfdot', kpar=list(sigma=i) )
print(svm3)
showSVM(svm3, df, i)
}
load("data3")
summary(train)
summary(test)
plot( train$x1, train$x2, pch=21, bg=c("red","blue")[train$y+1] )
plot(  test$x1,  test$x2, pch=21, bg=c("red","blue")[ test$y+1] )
lambda = 1
sig = 1
# training
svm = ksvm( y ~ x2 + x1, data=train, type="C-svc", C=lambda, kernel='rbfdot', kpar=list(sigma=sig) )
# visualize
showSVM(svm, train, FALSE, FALSE)
# prediction
y = predict(svm, test)
# compute accuracy
table(test$y, y)
sum(y==test$y)/length(test$y)
steps = c( 0.01, 0.03, 0.1, 0.3, 1, 3, 10 )
accuracy<-0
for ( i in 1:length(steps) ) {
print(paste('Step:',i, "out of 7"))
for ( j in 1:length(steps) ) {
lambda = steps[i]
sig    = steps[j]
# add code here
svm4 = ksvm( y ~ x2 + x1, data=train, type="C-svc", kernel='rbfdot', kpar=list(sigma=sig), C=lambda)
showSVM(svm4, train, i)
y_pred = predict(svm4, test)
acc=sum(y_pred==test$y)/length(test$y)
if (acc>accuracy){
accuracy<-acc
par<-c(lambda,sig)
}
print(paste("Accuracy:",accuracy))
}
}
accuracy
par
load("data2")
summary(df)
plot( df$x1, df$x2, pch=21, bg=c("red","blue")[df$y+1] )
library(nnet)
neurons = 2
net = nnet( y ~ x2 + x1, data=df, size = neurons, maxit = 1000, decay = 5e-4, trace = FALSE)
net
showNN <- function(net, frame)
{
# create a grid of points
hs <- 0.01
x1_min <- min( frame[,1]) - 0.01
x1_max <- max( frame[,1]) + 0.01
x2_min <- min( frame[,2]) - 0.01
x2_max <- max( frame[,2]) + 0.01
grid <- as.matrix(expand.grid( seq(x1_min, x1_max, by = hs), seq(x2_min, x2_max, by =hs) ))
grid <- data.frame( x1 = grid[,1], x2 = grid[,2] )
# predict with the neural network
y = predict(net, newdata = grid)
# visualize contour
x1 = seq(x1_min,x1_max,by=hs)
x2 = seq(x2_min,x2_max,by=hs)
y <- matrix(y, nrow = length(x1), byrow = FALSE)
contour( x1, x2, y, levels=0.5, lwd = 3, drawlabels = FALSE )
# visualize colors
#points( grid[,1], grid[,2], pch=".", cex=1, col = ifelse(y>0.5, "blue", "red") )
# visualize points
points( frame[,1], frame[,2], pch=21, cex=0.8, bg=c("red","blue")[frame[,3]+1] )
}
showNN(net, df)
for (i in seq(1,30)){
netw = nnet(y ~ x2 + x1, data=df, size = i, maxit = 1000, decay = 5e-4, trace = FALSE)
showNN(netw, df)
}
original_email = paste( readLines("emailSample1.txt"), collapse=" " )
print(original_email)
#install.packages("tm")
#install.packages("SnowballC")
library(tm)
library(SnowballC)
normalizeEmail <- function (email)
{
# normalization
email = tolower(email)
email = gsub('<[^<>]+>'              , ' '        , email, perl=TRUE)   # Strip all HTML
email = gsub('[0-9]+'                , 'number'   , email, perl=TRUE)   # Handle number
email = gsub('(http|https)://[^\\s]*', 'httpaddr' , email, perl=TRUE)   # Handle URLs
email = gsub('[^\\s]+@[^\\s]+'       , 'emailaddr', email, perl=TRUE)   # Handle email addresses
email = gsub('[$]+'                  , 'dollar'   , email, perl=TRUE)   # Handle $ sign
# stemming
email = Corpus( VectorSource(email) )
email = tm_map(email, removePunctuation)
#email = tm_map(email, removeWords, stopwords("english") )
email = tm_map(email, stemDocument)
email = tm_map(email, stripWhitespace)
# convert to a string
email = tm_map(email, PlainTextDocument)
email = as.character( email[[1]] )
return (email)
}
normalized_email = normalizeEmail(original_email)
print(original_email)
print("------")
print(normalized_email)
mapEmail <- function (email)
{
# Load Vocabulary
vocabList = read.table("vocab.txt")[,2]
# Init return value
indices = c();
# split words
email = unlist(strsplit(email, ' '))
# scan the email words
for( str in email )
{
i = grep( paste("^",str,"$", sep=""), vocabList )
indices = c( indices, i )
}
return (indices)
}
mapped_email = mapEmail(normalized_email)
print(mapped_email)
extractEmail <- function(email)
{
# Total number of words in the dictionary
n = 1899;
# You need to return the following variables correctly.
x = rep(0, n);
# ... ADD CODE HERE ...
return (x)
}
file_name = "emailSample1.txt"
# read an email
email = paste( readLines(file_name), collapse=" " )
# process the email
email = normalizeEmail(email)
email = mapEmail(email)
email = extractEmail(email)
# show statistics
cat( sprintf('Length of feature vector: %d\n', length(email)) );
cat( sprintf('Number of non-zero entries: %d\n', sum(email > 0)) );
load("spamTrain")
load("spamTest")
library(kernlab)
lambda = 0.1;
svm = ksvm( y ~ ., data=spamTrain, type="C-svc", kernel='vanilladot', C=lambda, scaled=FALSE )
svm
# prediction
y = predict(svm, spamTrain)
# compute accuracy
table( spamTrain$y, y )
acc = sum( y == spamTrain$y ) / length(y)
cat( sprintf('Training Accuracy: %2.3f%%\n', acc * 100) );
theta = colSums( coef(svm)[[1]] * spamTrain[unlist(alphaindex(svm)),1:ncol(spamTrain)-1])
vocabList = read.table("vocab.txt")[,2]
#install.packages("wordcloud")
library(wordcloud)
wordcloud( vocabList, theta, max.words=100, rot.per=0.2, colors=brewer.pal(4, "Dark2"))
d = sort(theta, decreasing=TRUE, index.return = TRUE)
i = d$ix[1:10]
processEmail <- function(x)
{
x = normalizeEmail(x)
x = mapEmail(x)
x = extractEmail(x)
return(x)
}
# add code here #
vocablist[i]
# add code here #
d[i]
# add code here #
vocabList[i]
# TODO: craft an email that will be classified as spam
spamTest2<-"Please click here, our website will guarantee you winning dollars for low price!"
spamTest2
processEmail(spamTest2)
procTest2<-processEmail(spamTest2)
procTest2
cat( sprintf('Is it spam (0=no, 1=yes)? %d\n', z) )
email = readFile('emailSample1.txt')  # CHANGE THE NAME WITH: emailSample1, emailSample3, spamSample1, spamSample2.
email = processEmail(email)
processEmail <- function(x)
{
x = normalizeEmail(x)
x = mapEmail(x)
x = extractEmail(x)
return(x)
}
readFile <- function(x)
{
email = paste( readLines(x), collapse=" " )
return(email)
}
email = readFile('emailSample1.txt')  # CHANGE THE NAME WITH: emailSample1, emailSample3, spamSample1, spamSample2.
email = processEmail(email)
z = predict( svm, rbind(email) )
cat( sprintf('Is it spam (0=no, 1=yes)? %d\n', z) )
email
email = readFile('emailSample3.txt')  # CHANGE THE NAME WITH: emailSample1, emailSample3, spamSample1, spamSample2.
email = readFile('spamSample2.txt')  # CHANGE THE NAME WITH: emailSample1, emailSample3, spamSample1, spamSample2.
email = processEmail(email)
z = predict( svm, rbind(email) )
cat( sprintf('Is it spam (0=no, 1=yes)? %d\n', z) )
email
library(kernlab)
lambda = 0.1;
svm = ksvm( y ~ ., data=spamTrain, type="C-svc", kernel='vanilladot', C=lambda, scaled=FALSE )
svm
svm = ksvm( y ~ ., data=spamTrain, type="C-svc", kernel='vanilladot', C=lambda, scaled=FALSE )
#install.packages("tm")
#install.packages("SnowballC")
library(tm)
library(SnowballC)
normalizeEmail <- function (email)
{
# normalization
email = tolower(email)
email = gsub('<[^<>]+>'              , ' '        , email, perl=TRUE)   # Strip all HTML
email = gsub('[0-9]+'                , 'number'   , email, perl=TRUE)   # Handle number
email = gsub('(http|https)://[^\\s]*', 'httpaddr' , email, perl=TRUE)   # Handle URLs
email = gsub('[^\\s]+@[^\\s]+'       , 'emailaddr', email, perl=TRUE)   # Handle email addresses
email = gsub('[$]+'                  , 'dollar'   , email, perl=TRUE)   # Handle $ sign
# stemming
email = Corpus( VectorSource(email) )
email = tm_map(email, removePunctuation)
#email = tm_map(email, removeWords, stopwords("english") )
email = tm_map(email, stemDocument)
email = tm_map(email, stripWhitespace)
# convert to a string
email = tm_map(email, PlainTextDocument)
email = as.character( email[[1]] )
return (email)
}
normalized_email = normalizeEmail(original_email)
original_email = paste( readLines("emailSample1.txt"), collapse=" " )
print(original_email)
#install.packages("tm")
#install.packages("SnowballC")
library(tm)
library(SnowballC)
normalizeEmail <- function (email)
{
# normalization
email = tolower(email)
email = gsub('<[^<>]+>'              , ' '        , email, perl=TRUE)   # Strip all HTML
email = gsub('[0-9]+'                , 'number'   , email, perl=TRUE)   # Handle number
email = gsub('(http|https)://[^\\s]*', 'httpaddr' , email, perl=TRUE)   # Handle URLs
email = gsub('[^\\s]+@[^\\s]+'       , 'emailaddr', email, perl=TRUE)   # Handle email addresses
email = gsub('[$]+'                  , 'dollar'   , email, perl=TRUE)   # Handle $ sign
# stemming
email = Corpus( VectorSource(email) )
email = tm_map(email, removePunctuation)
#email = tm_map(email, removeWords, stopwords("english") )
email = tm_map(email, stemDocument)
email = tm_map(email, stripWhitespace)
# convert to a string
email = tm_map(email, PlainTextDocument)
email = as.character( email[[1]] )
return (email)
}
normalized_email = normalizeEmail(original_email)
print(original_email)
print("------")
print(normalized_email)
mapEmail <- function (email)
{
# Load Vocabulary
vocabList = read.table("vocab.txt")[,2]
# Init return value
indices = c();
# split words
email = unlist(strsplit(email, ' '))
# scan the email words
for( str in email )
{
i = grep( paste("^",str,"$", sep=""), vocabList )
indices = c( indices, i )
}
return (indices)
}
mapped_email = mapEmail(normalized_email)
print(mapped_email)
extractEmail <- function(email)
{
# Total number of words in the dictionary
n = 1899;
# You need to return the following variables correctly.
x = rep(0, n);
# ... ADD CODE HERE ...
return (x)
}
file_name = "emailSample1.txt"
extractEmail <- function(email)
{
# Total number of words in the dictionary
n = 1899;
# You need to return the following variables correctly.
x = rep(0, n);
# ... ADD CODE HERE ...
x[email]=1
return (x)
}
file_name = "emailSample1.txt"
# read an email
email = paste( readLines(file_name), collapse=" " )
# process the email
email = normalizeEmail(email)
email = mapEmail(email)
email = extractEmail(email)
email
# show statistics
cat( sprintf('Length of feature vector: %d\n', length(email)) );
cat( sprintf('Number of non-zero entries: %d\n', sum(email > 0)) );
load("spamTrain")
load("spamTest")
library(kernlab)
lambda = 0.1;
svm = ksvm( y ~ ., data=spamTrain, type="C-svc", kernel='vanilladot', C=lambda, scaled=FALSE )
svm
# prediction
y = predict(svm, spamTrain)
# compute accuracy
table( spamTrain$y, y )
acc = sum( y == spamTrain$y ) / length(y)
cat( sprintf('Training Accuracy: %2.3f%%\n', acc * 100) );
theta = colSums( coef(svm)[[1]] * spamTrain[unlist(alphaindex(svm)),1:ncol(spamTrain)-1])
vocabList = read.table("vocab.txt")[,2]
#install.packages("wordcloud")
library(wordcloud)
wordcloud( vocabList, theta, max.words=100, rot.per=0.2, colors=brewer.pal(4, "Dark2"))
d = sort(theta, decreasing=TRUE, index.return = TRUE)
i = d$ix[1:10]
# add code here #
vocabList[i]
processEmail <- function(x)
{
x = normalizeEmail(x)
x = mapEmail(x)
x = extractEmail(x)
return(x)
}
readFile <- function(x)
{
email = paste( readLines(x), collapse=" " )
return(email)
}
email = readFile('spamSample2.txt')  # CHANGE THE NAME WITH: emailSample1, emailSample3, spamSample1, spamSample2.
email = processEmail(email)
email
z = predict( svm, rbind(email) )
cat( sprintf('Is it spam (0=no, 1=yes)? %d\n', z) )
# TODO: craft an email that will be classified as spam
spamTest2<-"Please click here, our website will guarantee you winning dollars for low price!"
procTest2<-processEmail(spamTest2)
procTest2
predict(svm, rbind(procTest2))
predict(svm, rbind(procTest2))
values = c(...)           # complete the code
values = c(0.01,0.03,0.1,0.3,1,3,10)          # complete the code
for( lambda in values )
for( lambda in values ){
svm = ksvm(...)        # complete the code
y = predict(svm, ...)  # complete the code
acc = ...              # complete the code
cat( sprintf('Training Accuracy: %2.3f%%\n', acc * 100) );
}
for( lambda in values ){
svm = ksvm(...)        # complete the code
y = predict(svm, ...)  # complete the code
acc = ...              # complete the code
cat( sprintf('Training Accuracy: %2.3f%%\n', acc * 100) );
}
svm = ksvm(y ~ ., data=spamTrain, type="C-svc", kernel='vanilladot', C=lambda, scaled=FALSE)        # complete the code
for( lambda in values ){
svm = ksvm(y ~ ., data=spamTrain, type="C-svc", kernel='vanilladot', C=lambda, scaled=FALSE)        # complete the code
y = predict(svm, spamTest)  # complete the code
acc = sum(y==spamTest$y)/length(y)              # complete the code
cat( sprintf('Training Accuracy: %2.3f%%\n', acc * 100) );
}
