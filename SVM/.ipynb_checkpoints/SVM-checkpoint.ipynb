{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    "\n",
    "------\n",
    "-  [1. Support vector machines](#Support-vector-machines)\n",
    "    - [1.1 Linear SVM](#Linear-SVM)\n",
    "        - [Question 1](#Question-1)\n",
    "        \n",
    "    - [1.2 Nonlinear SVM](#Nonlinear-SVM)\n",
    "        - [Question 2](#Question-2)\n",
    "    - [1.3 Validation](#Validation)\n",
    "        - [Question 3](#Question-3)\n",
    "\n",
    "- [2. Spam classification](#Spam-classification)\n",
    "    - [2.1 Preprocessing](#Preprocessing)\n",
    "    - [2.2 Vocabulary list](#Vocabulary-list)\n",
    "    - [2.2 Feature extraction](#Feature-extraction)\n",
    "        - [Question 4](#Question-4)\n",
    "    - [2.3 SVM training](#SVM-training)\n",
    "        - [Question 5](#Question-5)\n",
    "        - [Question 6](#Question-6)\n",
    "        - [Question 7](#Question-7)\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support vector machines\n",
    "\n",
    "In the first part of this laboratory session, you will gain an intuition of \n",
    "- how SVMs work\n",
    "- how to use a Gaussian kernel\n",
    "- how to set the associated parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM\n",
    "\n",
    "Let us begin with a 2D dataset that can be separated by a linear split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "load(\"data1\")\n",
    "summary(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the data as points in a two-dimensional space (using a different color according to which class they belong to). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot( df$x1, df$x2, pch=21, bg=c(\"red\",\"blue\")[df$y+1] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the scatter plot, the position of the samples suggests a natural separation. \n",
    "\n",
    "However, there is an outlier on the far left: as part of this first exercise, you will also see how this outlier affects the SVM.\n",
    "\n",
    "In order to train a SVM, you need the package **kernlab**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#install.packages(\"kernlab\")\n",
    "\n",
    "library(kernlab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you can train a SVM with the function **ksvm**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svm = ksvm( y ~ x2 + x1, data=df, type=\"C-svc\", kernel='vanilladot' )\n",
    "svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To gain a better insight into the classification, you can visualize how the trained SVM splits the data. \n",
    "\n",
    "The function defined below allows you to do exactly that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "showSVM <- function(svm, frame, margin=TRUE, support=TRUE)\n",
    "{\n",
    "    # create a grid of points\n",
    "    hs <- 0.01\n",
    "    x1_min <- min( frame[,1]) - 0.01 \n",
    "    x1_max <- max( frame[,1]) + 0.01\n",
    "    x2_min <- min( frame[,2]) - 0.01 \n",
    "    x2_max <- max( frame[,2]) + 0.01\n",
    "    grid <- as.matrix(expand.grid( seq(x1_min, x1_max, by = hs), seq(x2_min, x2_max, by =hs) ))\n",
    "    grid <- data.frame( x1 = grid[,1], x2 = grid[,2] )\n",
    "    \n",
    "    # predict with the SVM\n",
    "    y = predict(svm, newdata = grid, type=\"decision\")\n",
    "    \n",
    "    # visualize points\n",
    "    plot( frame[,1], frame[,2], pch=21, cex=0.8, bg=c(\"red\",\"blue\")[frame[,3]+1] )\n",
    "    \n",
    "    # highlight the support vectors\n",
    "    idx = unlist( alphaindex(svm) )\n",
    "    if(support)\n",
    "        points( frame[idx,1], frame[idx,2], col=c(\"red\",\"blue\")[frame[idx,3]+1], pch=5, cex=1.5)\n",
    "    \n",
    "    # visualize the decision hyperplane\n",
    "    x1 = seq(x1_min,x1_max,by=hs)\n",
    "    x2 = seq(x2_min,x2_max,by=hs)\n",
    "    y <- matrix(y, nrow = length(x1), byrow = FALSE)\n",
    "    contour( x1, x2, y, levels=0, lwd=3, lty=1, drawlabels = FALSE, add=TRUE )\n",
    "        \n",
    "    # visualize the margins\n",
    "    if(margin)\n",
    "        contour( x1, x2, y, levels=c(-1,1), lwd = 1, lty=2, drawlabels = FALSE, add=TRUE )    \n",
    "        \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoke the function **showSVM** to visualize the SVM you just trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "showSVM(svm, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can use the built-in function **plot**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(svm, data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the SVM has a parameter **C** which controls the penalty for misclassified training samples. \n",
    "\n",
    "***A large value of C tells the SVM to try to classify all the examples correctly.***\n",
    "    \n",
    "By default, the function **ksvm** sets **C = 1**, but you can specify a different value with the input **C**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lambda = 1;\n",
    "\n",
    "svm = ksvm( y ~ x2 + x1, data=df, type=\"C-svc\", kernel='vanilladot', C=lambda, scaled=FALSE )\n",
    "svm\n",
    "\n",
    "showSVM(svm, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 1\n",
    "Your task is to try different values of **C** on this dataset. \n",
    " - Can you set **C** such that all samples are correctly classified? \n",
    " - How does the margin change by increasing/reducing **C**? \n",
    " - In your opinion, should the value of **C** be small or large in order to have a classifier that performs well on new data?\n",
    " \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear SVM\n",
    "\n",
    "Let us switch to a 2D dataset that can be only separated by a nonlinear split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "load(\"data2\")\n",
    "\n",
    "summary(df)\n",
    "\n",
    "plot( df$x1, df$x2, pch=21, bg=c(\"red\",\"blue\")[df$y+1] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the figure, you can observe that there is no linear split that separates the samples. However, by using the kernel trick, you will be able to learn a nonlinear SVM that can perform reasonably well for the dataset. So far, you have used the function **ksvm** with a linear kernel, but you can specify a different one through the input parameter **kernel**. Although there are several kernels available, you will be using SVMs with Gaussian kernels (option **'rbfdot'**). \n",
    "\n",
    "***It is always best to standardize the data when using a nonlinear kernel.***\n",
    "\n",
    "This is done automatically by the function **ksvm** : just leave the input parameter **scaled** unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm = ksvm( y ~ x2 + x1, data=df, type=\"C-svc\", kernel='rbfdot' )\n",
    "svm\n",
    "\n",
    "showSVM(svm, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the Gaussian kernel measures the similarity between a pair of examples, and it is parameterized by the bandwidth $\\gamma=1/\\sigma^2$, which determines how fast the similarity metric decreases to $0$ as the examples are further apart.\n",
    "\n",
    "*** A large value of $\\sigma$ tells the SVM to closely follow the samples. ***\n",
    "\n",
    "You can specify a different value of $\\sigma$ with the option **kpar**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sig = 0.2\n",
    "\n",
    "svm = ksvm( y ~ x2 + x1, data=df, type=\"C-svc\", kernel='rbfdot', kpar=list(sigma=sig) )\n",
    "svm\n",
    "\n",
    "showSVM(svm, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 2\n",
    "Your task is to try different values of $\\sigma$ on this dataset. \n",
    "- Can you set $\\sigma$ such that most of the samples are correctly classified? \n",
    "- In terms of fitting, what does it happen when $\\sigma$ is too small or too big? \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "In this last exercise, you will gain more practical skills on how to use a SVM with the Gaussian kernel. \n",
    "\n",
    "Load the third dataset (which is non-separable): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "load(\"data3\")\n",
    "\n",
    "summary(train)\n",
    "summary(test)\n",
    "\n",
    "plot( train$x1, train$x2, pch=21, bg=c(\"red\",\"blue\")[train$y+1] )\n",
    "plot(  test$x1,  test$x2, pch=21, bg=c(\"red\",\"blue\")[ test$y+1] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided dataset contains two frames: \n",
    " - **train** holds the training set\n",
    " - **test**  holds the validation set. \n",
    "\n",
    "Remember that the validation set is not used for training the SVM, but only for evaluating the classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lambda = 1\n",
    "sig = 1\n",
    "\n",
    "# training\n",
    "svm = ksvm( y ~ x2 + x1, data=train, type=\"C-svc\", C=lambda, kernel='rbfdot', kpar=list(sigma=sig) )\n",
    "\n",
    "# visualize\n",
    "showSVM(svm, train, FALSE, FALSE)\n",
    "\n",
    "# prediction\n",
    "y = predict(svm, test)\n",
    "\n",
    "# compute accuracy\n",
    "table(test$y, y)\n",
    "sum(y==test$y)/length(test$y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 3\n",
    "Your task is to find the values of $\\lambda$ and $\\sigma$ such that the trained SVM obtains the best accuracy on the validation set. For both parameters, we suggest trying values in multiplicative steps: $0.01, 0.03, 0.1, 0.3, 1, 3, 10$. Note that you should try all possible pairs of values, so you will end up training a total of $7^2 = 49$ different SVMs. What are the best values of $\\lambda$ and $\\sigma$?\n",
    "\n",
    "*Hint :* To automatize the process, you can use two nested loops (see below for an example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "steps = c( 0.01, 0.03, 0.1, 0.3, 1, 3, 10 )\n",
    "\n",
    "acc = matrix(0, length(steps), length(steps) )\n",
    "\n",
    "for ( i in 1:length(steps) ) {\n",
    "    for ( j in 1:length(steps) ) {\n",
    "        lambda = steps[i]\n",
    "        sig    = steps[j]\n",
    "        \n",
    "        # training\n",
    "        svm = ksvm( y ~ x2 + x1, data=train, type=\"C-svc\", C=lambda, kernel='rbfdot', kpar=list(sigma=sig) )\n",
    "        \n",
    "        # prediction\n",
    "        y = predict(svm, test)\n",
    "        \n",
    "        # accuracy\n",
    "        acc[i,j] = sum(y==test$y)/length(test$y)\n",
    "    }\n",
    "}\n",
    "\n",
    "acc\n",
    "\n",
    "# TODO: find i and j such that acc[i,j] is the maximum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many email services today provide spam filters that are able to classify emails with high accuracy. In this part of the assignment, you will use SVMs to classify if a given email is spam or non-spam. The dataset is based on a subset of the *SpamAssassin Public Corpus*: http://spamassassin.apache.org/publiccorpus/\n",
    "\n",
    "Before starting on a machine learning task, it is usually insightful to take a look at examples from the dataset. Hereafter, you can see a sample email that contains a URL, an email address, numbers, and dollar amounts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "original_email = paste( readLines(\"emailSample1.txt\"), collapse=\" \" )\n",
    "\n",
    "print(original_email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "While many emails would contain similar types of entities (e.g., numbers, URLs, or email addresses), the specific entities (e.g., the specific URL or dollar amount) will be different in almost every email. Therefore, one method often employed in processing emails is to normalize these values, so that all URLs are treated the same, all numbers are treated the same, etc. For example, we could replace each URL in the email with the unique string \"httpaddr\". This has the effect of letting the spam classifier make a decision based on the presence of any URL, rather than a specific URL. This typically improves the performance of a spam classifier, since spammers randomize the URLs, and thus the odds of seeing any particular URL again in a new piece of spam is very small. \n",
    "\n",
    "In the function **normalizeEmail** (given below), we implemented the following preprocessing steps.\n",
    "\n",
    "- **Lower-casing**: The entire email is converted into lower case, so that capitalization is ignored (e.g., *IndIcaTE* is treated the same as *Indicate*).\n",
    "\n",
    "- **Stripping HTML**: All HTML tags are removed from the emails. Many emails often come with HTML formatting; we remove all the HTML tags, so that only the content remains.\n",
    "\n",
    "- **Normalizing URLs**: All URLs are replaced with the text *httpaddr*.\n",
    "\n",
    "- **Normalizing Emails**: All email addresses are replaced with the text *emailaddr*.\n",
    "\n",
    "- **Normalizing Numbers**: All numbers are replaced with the text *number*.\n",
    "\n",
    "- **Normalizing Dollars**: All dollar signs ($) are replaced with the text *dollar*.\n",
    "\n",
    "- **Word Stemming**: Words are reduced to their stemmed form. For example, *discount*, *discounts*, *discounted* and\n",
    "*discounting* are all replaced with \"discount\". Sometimes, the Stemmer actually strips off additional characters from the end, so *include*, *includes*, *included*, and *including* are all replaced with \"includ\".\n",
    "\n",
    "- **Removal of non-words**: Non-words and punctuation have been removed. All white spaces (tabs, newlines, spaces) have all been trimmed to a single space character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#install.packages(\"tm\")\n",
    "#install.packages(\"SnowballC\")\n",
    "library(tm)\n",
    "library(SnowballC)   \n",
    "\n",
    "normalizeEmail <- function (email)\n",
    "{      \n",
    "    # normalization\n",
    "    email = tolower(email)\n",
    "    email = gsub('<[^<>]+>'              , ' '        , email, perl=TRUE)   # Strip all HTML\n",
    "    email = gsub('[0-9]+'                , 'number'   , email, perl=TRUE)   # Handle number\n",
    "    email = gsub('(http|https)://[^\\\\s]*', 'httpaddr' , email, perl=TRUE)   # Handle URLs\n",
    "    email = gsub('[^\\\\s]+@[^\\\\s]+'       , 'emailaddr', email, perl=TRUE)   # Handle email addresses\n",
    "    email = gsub('[$]+'                  , 'dollar'   , email, perl=TRUE)   # Handle $ sign\n",
    "    \n",
    "    # stemming\n",
    "    email = Corpus( VectorSource(email) )\n",
    "    email = tm_map(email, removePunctuation)\n",
    "    #email = tm_map(email, removeWords, stopwords(\"english\") )\n",
    "    email = tm_map(email, stemDocument)\n",
    "    email = tm_map(email, stripWhitespace)\n",
    "    \n",
    "    # convert to a string\n",
    "    email = tm_map(email, PlainTextDocument)  \n",
    "    email = as.character( email[[1]] )\n",
    "    \n",
    "    return (email)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of these preprocessing steps is shown below. \n",
    "\n",
    "While preprocessing has left word fragments and non-words, this form turns out to be much easier to work with for performing feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normalized_email = normalizeEmail(original_email)\n",
    "\n",
    "print(original_email)\n",
    "print(\"------\")\n",
    "print(normalized_email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary list\n",
    "\n",
    "After preprocessing the emails, we have a list of words for each email. The next step is to choose which words we would like to use in our classifier and which we would want to leave out. For this assignment, we have chosen only the most frequently occurring words as our set of words considered (the vocabulary list). Since words that occur rarely in the training set are only in a few emails, they might cause the model to over-fit our training set. \n",
    "\n",
    "The complete vocabulary list is in the file \"vocab.txt\", and also shown below:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1 aa           85 anymor       915 knew           1896 yourself\n",
    "2 ab           86 anyon        916 know           1897 zdnet\n",
    "3 abil         87 anyth        917 knowledg       1898 zero\n",
    "...            ...             ...                1899 zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our vocabulary list was selected by choosing all words which occur at least a 100 times in the spam corpus, resulting in a list of 1899 words. In practice, a vocabulary list with about 10'000 to 50'000 words is often used. Given the vocabulary list, we can now map each word in the preprocessed emails into a list of word indices that contains the index of the word in the vocabulary list. \n",
    "\n",
    "In the function **mapEmail** (given below), we implemented the code to perform the mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mapEmail <- function (email)\n",
    "{   \n",
    "    # Load Vocabulary\n",
    "    vocabList = read.table(\"vocab.txt\")[,2]\n",
    "    \n",
    "    # Init return value\n",
    "    indices = c();\n",
    "    \n",
    "    # split words\n",
    "    email = unlist(strsplit(email, ' '))\n",
    "    \n",
    "    # scan the email words\n",
    "    for( str in email )\n",
    "    {\n",
    "        i = grep( paste(\"^\",str,\"$\", sep=\"\"), vocabList )\n",
    "        \n",
    "        indices = c( indices, i )\n",
    "    }\n",
    "    \n",
    "    return (indices)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hereafter, you can see the mapping for the sample email previously considered. Specifically, in the sample email, the word *anyone* was first normalized to *anyon* and then mapped onto the index 86 in the vocabulary list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mapped_email = mapEmail(normalized_email)\n",
    "\n",
    "print(mapped_email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "You will now implement the feature extraction that converts each email into a vector $x\\in\\mathbb{R}^K$, where $K$ is the number of words in the vocabulary list. Specifically, if the $i$-th word in the vocabulary is present in the email, you have $x_i=1$, otherwise $x_i = 0$. Thus, for a typical email, the feature vector is like:\n",
    "\n",
    "$$\n",
    "x = \\left[0\\;\\dots\\;1\\;0\\;\\dots\\;1\\;0\\;\\dots\\;0\\right]^\\top.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 4\n",
    "\n",
    "Your task is to complete the code in the function **extractEmail** to generate a feature vector for an email, given the word indices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extractEmail <- function(email)\n",
    "{\n",
    "    # Total number of words in the dictionary\n",
    "    n = 1899;\n",
    "\n",
    "    # You need to return the following variables correctly.\n",
    "    x = rep(0, n);\n",
    "    \n",
    "    # ... ADD CODE HERE ...\n",
    "    \n",
    "    return (x)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Once you have done it, test the function on a sample email as follows. You should see that the feature vector had length 1899 and 45 non-zero entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_name = \"emailSample1.txt\"\n",
    "\n",
    "# read an email\n",
    "email = paste( readLines(file_name), collapse=\" \" )\n",
    "\n",
    "# process the email\n",
    "email = normalizeEmail(email)\n",
    "email = mapEmail(email)\n",
    "email = extractEmail(email)\n",
    "\n",
    "# show statistics\n",
    "cat( sprintf('Length of feature vector: %d\\n', length(email)) );\n",
    "cat( sprintf('Number of non-zero entries: %d\\n', sum(email > 0)) );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM training\n",
    "\n",
    "You are now ready to train a SVM for spam classification. We have already preprocessed a **training set** and a **validation set**, where each original email was processed using the previous functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load(\"spamTrain\")\n",
    "load(\"spamTest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the training set, you can proceed to train a linear SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "library(kernlab)\n",
    "\n",
    "lambda = 0.1;\n",
    "svm = ksvm( y ~ ., data=spamTrain, type=\"C-svc\", kernel='vanilladot', C=lambda, scaled=FALSE )\n",
    "svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and compute the classification accuracy on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prediction\n",
    "y = predict(svm, spamTrain)\n",
    "\n",
    "# compute accuracy\n",
    "table( spamTrain$y, y )\n",
    "acc = sum( y == spamTrain$y ) / length(y)\n",
    "\n",
    "cat( sprintf('Training Accuracy: %2.3f%%\\n', acc * 100) );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 5\n",
    "\n",
    "We would like to know which words the trained SVM thinks are the most predictive of spam. Remember that an email $x\\in\\mathbb{R}^K$ is classified with the rule \n",
    "\n",
    "$$y = \\operatorname{sign}\\left( \\theta_0 + \\theta^\\top x \\right)$$\n",
    "\n",
    "Hence, the largest positive components in the vector $\\theta$ correspond to the most indicative words of spam. \n",
    "\n",
    "However, the function **ksvm** solves the dual SVM formulation, returning a vector $\\alpha$ which is related to $\\theta$ by the following equation:\n",
    "$$\n",
    "\\theta = \\sum_{n=1}^N \\alpha_n \\, y_n \\, x_n\n",
    "$$\n",
    "where $x_n \\in \\mathbb{R}^K$ is a training sample, $y_n \\in \\{-1,1\\}$ denotes the corresponding class, and $N$ is the size of the training set.\n",
    "\n",
    "The above formula can be translated in R as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta = colSums( coef(svm)[[1]] * spamTrain[unlist(alphaindex(svm)),1:ncol(spamTrain)-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from which we can infer the most indicative words of spam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabList = read.table(\"vocab.txt\")[,2]\n",
    "\n",
    "#install.packages(\"wordcloud\")\n",
    "library(wordcloud)\n",
    "\n",
    "wordcloud( vocabList, theta, max.words=100, rot.per=0.2, colors=brewer.pal(4, \"Dark2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which are the words associated to the 10 biggest components of $\\theta$ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = sort(theta, decreasing=TRUE, index.return = TRUE)\n",
    "i = d$ix[1:10]\n",
    "\n",
    "# add code here #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 6\n",
    "Now that you have trained a spam classifier, you can start trying it out on your own emails. We have included two non-spam emails (*emailSampleXXX.txt*) and two spam emails (*spamSampleXXX.txt*). The following code illustrates how to classify an email with the trained SVM.\n",
    "- Does the classifier get right the other emails we provided? \n",
    "- Based on the answer from the previous question, can you craft an email that is classified as spam?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "processEmail <- function(x)\n",
    "{\n",
    "    x = normalizeEmail(x)\n",
    "    x = mapEmail(x)\n",
    "    x = extractEmail(x)\n",
    "    return(x)\n",
    "}\n",
    "\n",
    "readFile <- function(x)\n",
    "{\n",
    "    email = paste( readLines(x), collapse=\" \" )   \n",
    "    return(email)\n",
    "}\n",
    "\n",
    "email = readFile('emailSample1.txt')  # CHANGE THE NAME WITH: emailSample1, emailSample3, spamSample1, spamSample2.\n",
    "email = processEmail(email)\n",
    "\n",
    "z = predict( svm, rbind(email) )\n",
    "\n",
    "cat( sprintf('Is it spam (0=no, 1=yes)? %d\\n', z) )\n",
    "\n",
    "# TODO: craft an email that will be classified as spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 7\n",
    "Finally, your task is to find the value of $\\lambda$ leading to the best accuracy on the validation set. \n",
    "\n",
    "You should see that the classifier gets a test accuracy of 98.5\\% or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "values = c(...)           # complete the code\n",
    "\n",
    "for( lambda in values )\n",
    "{\n",
    "    svm = ksvm(...)        # complete the code\n",
    "    \n",
    "    y = predict(svm, ...)  # complete the code\n",
    "    \n",
    "    acc = ...              # complete the code\n",
    "    \n",
    "    cat( sprintf('Training Accuracy: %2.3f%%\\n', acc * 100) );\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "interactive_sols": {
   "cbx_id": 1
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "toc": {
   "toc-wrapper_display": "block",
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 4
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
